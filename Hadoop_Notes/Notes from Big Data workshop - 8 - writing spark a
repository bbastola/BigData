Notes from Big Data workshop - 8 - writing spark applications using scala

Problem statement: Compute total revenue of each day

1. create a new scala/sbt project "retail" un intellij
2. under build.sbt add libraryDepencies: libraryDependencies += "org.apache.spark" % "spark-core_2.10" % "1.6.2" and save the file->it will resolve necessary repos. Should looks like this:
		name := "retail"

		version := "1.0"

		scalaVersion := "2.10.6"

		libraryDependencies += "org.apache.spark" % "spark-core_2.10" % "1.6.2"
3. go to the location where "retail" is located locally.
4. run: sbt console
5. import org.apache.spark.{SparkConf, SparkContext}
6. val conf = new SparkConf().setAppName("Daily Revenue calculation").setMaster("local")
7. val sc = new SparkContext(conf)
8. val orders = sc.textFile("/Users/bbastola/Documents/Github/Dataset/retail_db/orders")
9. orders.take(10).foreach(println)
10. val orders_filtered = orders.filter(rec => rec.contains("COMPLETE") || rec.contains("CLOSED"))
11. orders_filtered.take(10).foreach(println)
12. val orders_map = orders_filtered.map(rec => (rec.split(",")(0).toInt, rec.split(",")(1)))

13. val order_items = sc.textFile("/Users/bbastola/Documents/Github/Dataset/retail_db/order_items")
14. order_items.take(10).foreach(println)
15. val order_items_filtered = order_items.map(rec => (rec.split(",")(1).toInt, rec.split(",")(4).toDouble))

16. val final_join = orders_map.join(order_items_filtered)
17. final_join.take(10).foreach(println)

18. val ordersJoinMap = final_join.map(rec => rec._2)

19. val ordersJoinMapGBK = ordersJouinMap.groupByKey()
20. val ordersJoinMapGBKMap = ordersJoinMapGBK.map(rec => (rec._1, rec._2.sum))
21. ordersJoinMapGBKMap.take(10).foreach(println)

				OR
19. val ordersJoinMapRBK = ordersJouinMap.reduceByKey((agg, value) => agg + value)
20. ordersJoinMapRBK.collect().foreach(println)






