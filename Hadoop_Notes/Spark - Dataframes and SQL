Spark - Dataframes and SQL

1. First defince dependencies in build.sbt for sparkSQL, spark-hive context

	libraryDependencies += "org.apache.spark" % "spark-core_2.10" % "1.6.2"
	libraryDependencies += "org.apache.spark" % "spark-sql_2.10" % "1.6.2"
	libraryDependencies += "org.apache.spark" % "spark-hive_2.10" % "1.6.2"

2. launch console using sbt console

3. Import necessary packages and configure conf and sc

	import org.apache.spark.{SparkConf, SparkContext}
    import org.apache.spark.sql._
    import org.apache.spark.sql.functions._

	val conf = new SparkConf().setAppName("Data Frames").setMaster("local")
	val sc = new SparkContext(conf)
	val sqlContext = new SQLContext(sc)
	sqlContext.setConf("spark.sql.shuffle.partitions", "4") //here we are assigning number of partitions to 4
	import sqlContext.implicits._

4. Create a case class

	case class Orders(
	order_id: Int,
	order_date: String,
	order_cusomter_id: Int,
	order_status: String)

	case class OrderItems(
	order_item_id: Int,
	order_item_order_id: Int,
	order_item_product_id: Int,
	order_item_quantity: Int,
	order_item_subtotal: Double,
	order_item_product_price: Double)

5. Now we have to creata RDD by importing the data

	val orders = sc.textFile("/Users/bbastola/Documents/Github/Dataset/retail_db/orders")
	val orderItems = sc.textFile("/Users/bbastola/Documents/Github/Dataset/retail_db/order_items")


6. now we have to convert the RDD into Dataframes. Note: we are mapping RDD by invoking apply method of case class
	
	val ordersDf = orders.map(rec => {
	val r = rec.split(",")
	Orders(r(0).toInt, r(1), r(2).toInt, r(3))
}).toDF

	val orderItemsDF = order_items.map(rec => {
	val r = rec.split(",")
	OrderItems(r(0).toInt, r(1).toInt, r(2).toInt, r(3).toInt, r(4).toDouble, r(5).toDouble)
	}).toDF

7. to print schema
	ordersDF.printSchema()

8. to see sample data
	ordersDF.show()

9. to select few fileds only
	ordersDF.select("order_id", "order_date").show()

10. to use filter
	ordersDF.filter(ordersDF("order_status") === "complete").show()
	ordersDF.filter(ordersDF("order_status") === "complete" or ordersDF("order_status") === "closed" ).show()

10. to run queries
	ordersDF.registerTempTable("orders")
	sqlContext.sql("Select * from orders limit 10").collect().foreach(println)

-----------------------------------------------------------------------------------------------------

Now from above materials let's create a use case:
* Calculate the daily revenue (On completed and closed orders)

1. lets create a df for filtered data

	val ordersFiltered = ordersDF.filter(ordersDF("order_status") === "complete" or ordersDF("order_status") === "closed")

2. now we have to join two tables
	
	val ordersJoin = ordersFiltered.join(orderItemsDf, ordersFiltered("order_id") === orderItemsDF("order_items_order_id"))

3. ordersJoin.printSchema()

5. now we have to group by order data, then apply aggregate function on sum of closed/complete orders
	
	val dailyRevenue = ordersJoin.groupBy("order_date").
						agg(sum("order_item_subtotal")).
						show()

Very Important concept. By default when you run queries in DF and if it requires shuffle, it will take 200 partitions to complete a task. 200 task could be very inefficient depending on the date size.
So you can change that by setting the following parameters before running the job

sqlContext.setConf("spark.sql.shuffle.partitions", "4") //here we are assigning number of partitions to 4

Now if you run that query again it will use only 4 partitions
	
	val dailyRevenue = ordersJoin.groupBy("order_date").
						agg(sum("order_item_subtotal")).
						show()

----------------------------------------------------------------------------------------------------

Now let's learn how to do this in spark sql rather than using dataframes


	