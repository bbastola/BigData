//Solution for problem statement 1

pyspark --master yarn --conf spark.ui.port=12888 --num-executors 2 --excutor-memory 512M
sc.setLogLevel("ERROR")

//load data from hdfs to spark session
orders = sc.textFile("/public/retail_db/orders")
order_items = sc.textFile("/public/retail_db/order_items")

//filter orders RDD to only have COMPLETED and CLOSED orders
ordersFilter = orders.filter(lambda rec: rec.split(",")[3] in ["COMPLETE", "CLOSED"])

//Extract required columns to make key value for joining
ordersMap = ordersFilter.map(lambda rec: (int(rec.split(",")[0]), rec.split(",")[1]))
orderItemsMap = order_items.map(lambda rec: (int(rec.split(",")[1]), (int(rec.split(",")[2]), float(rec.split(",")[4]))))

//join two rdds
itemsJoined = ordersMap.join(orderItemsMap)

//discard orderid since we won't need it. Also making orderDate and product id as key since we're 
computing daily revenue by product
(65536, (u'2014-05-16 00:00:00.0', (957, 299.98)))  --> This is the data where we have to work with
intermediateValue = itemsJoined.map(lambda rec: ((rec[1][0], int(rec[1][1][0])), rec[1][1][1]))


//now we can use reduceByKey to calculate subtotal
dailyRevenuePerProduct = intermediateValue.reduceByKey(lambda x, y: round((x+y),2))
//we have to modify this date so that productID is key and others are value
dailyRevenuePerProductMap = dailyRevenuePerProduct.map(lambda r: (int(r[0][1]), (r[0][0], float(r[1]))))

//now we need to join this data with prodcut table to get the name of products.
//First we need to load the data from local location and parellelize to convert to RDD
productsRaw = open("/data/retail_db/products/part-00000").read().splitlines()
product = sc.parallelize(productsRaw)

//now we need to extract the needed coloumn so that we can join
productMap = product.map(lambda rec: (int(rec.split(",")[0]), rec.split(",")[2]))

//joining dailyRevenue with productMap
dailyRevenuPerProductName = dailyRevenuePerProductMap.join(productMap)

//now we need to sort the data and extract only needed columns as per problem statement and save with appropriate data format

(24, ((u'2013-12-04 00:00:00.0', 159.98), 'Elevation Training Mask 2.0'))  --> current data form

//Now save as textFile
dailyRevenueSortedMap = dailyRevenuPerProductName.map(lambda r: ((r[1][0][0], -r[1][0][1]), r[1][0][0] + "," + str(r[1][0][1]) + "," + (r[1][1])))
dailyRevenueSortByDate = dailyRevenueSortedMap.sortByKey()
finalResult = dailyRevenueSortByDate.map(lambda rec: rec[1])
for i in finalResult.take(20): print i  --> just to see the current data
finalResult.saveAsTextFile("/user/bbastola/daily_revenue_txt_python")

//Save as avro format

pass this argument while launching pyspark

--packages com.databricks:spark-avro_2.10:2.0.1  -> group id: com.databrics  artifactsID: spark-avro_2.10  version: 2.0.1
	OR
--jars <location of jar file>

dailyRevenuePerProductNameMap = dailyRevenuPerProductName.map(lambda rec: ((rec[1][0][0], -float(rec[1][0][1])), rec[1][0][0], rec[1][0][1], rec[1][1]))

dailyRevenuePerProductNameSorted = dailyRevenuePerProductNameMap.sortByKey().map(lambda rec: rec[1])

dailyRevenueDF = dailyRevenuePerProductNameSorted.toDF(schema=["order_Date", "daily_revenue", "product_name"])
dailyRevenueDF.show() --> to preview the data

dailyRevenueDF.save("/user/bbastola/daily_revenue_avro_python", "com.databricks.spark.avro")


























