Details - Duration 15 to 20 minutes

Data is available in HDFS file system under /public/crime/csv
Structure of data (ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)
File format - text file
Delimiter - “,” (use regex while splitting split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1), as there are some fields with comma and enclosed using double quotes.

Get top 3 crime types based on number of incidents in RESIDENCE area using “Location Description”
Store the result in HDFS path "/user/<YOUR_USER_ID>/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA"
Output Fields: Crime Type, Number of Incidents
Output File Format: JSON
Output Delimiter: N/A
Output Compression: No

crimeData = sc.textFile("/public/crime/csv")
crimeDataHeader = crimeData.first()
crimeDataNoHeader = crimeData.filter(lambda rec: rec != crimeDataHeader)

crimeDataFilter = crimeData.filter(lambda rec: rec.split(",")[7] in ['RESIDENCE'])
crimeDataFilterMap = crimeDataFilter.map(lambda rec: (rec.split(",")[5], 1))

crimeDataReduceByKey = crimeDataFilterMap.reduceByKey(lambda x, y: x + y)
crimeDataMap = crimeDataReduceByKey.map(lambda x: (int(x[1]), x[0]))

crimeDataSort = crimeDataMap.sortByKey(False)
top3Crime = crimeDataSort.map(lambda rec: (rec[1], rec[0])).take(3)


top3CrimeDF = sc.parallelize(top3Crime).toDF(["crim_type", "number_of_Incidents"])

top3CrimeDF.write.json("/user/bbastola/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA")

validation:

sqlContext.read.json("/user/bbastola/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA").show()


********** Now with Spark SQL *******************

crimeData = sc.textFile("/public/crime/csv")
crimeDataHeader = crimeData.first()
crimeDataNoHeader = crimeData.filter(lambda rec: rec != crimeDataHeader)

from pyspark.sql import Row
crimeMap = crimeDataNoHeader.map(lambda x: Row(crime_type=x.split(",")[5], location=x.split(",")[7])).toDF()
crimeMap.registerTempTable("crime_table")

top3Crime = sqlContext.sql("select crime_type, count(1) number_of_incident from crime_table \
where location in ('RESIDENCE') \
group by crime_type \
order by number_of_incident desc \
limit 3")

top3Crime.write.json("/user/bbastola/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA_sql")

validation:

sqlContext.read.json("/user/bbastola/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA_sql").show()




