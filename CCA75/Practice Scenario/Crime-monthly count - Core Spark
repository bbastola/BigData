Details - Duration 40 minutes

Choose language of your choice Python or Scala
Data is available in HDFS file system under /public/crime/csv
You can check properties of files using hadoop fs -ls -h /public/crime/csv
Structure of data (ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,
District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)

5679860,HN487201,07/24/2007 11:10:00 PM,116XX S RACINE AVE,0560,ASSAULT,SIMPLE,RESIDENCE,false,true,0524,
005,34,53,08A,1170418,1827643,2007,04/15/2016 08:55:02 AM,41.682532092,-87.651799575,"(41.682532092, -87.651799575)"

File format - text file
Delimiter - “,”

Get monthly count of primary crime type, sorted by month in ascending and number of crimes per type in descending order
Store the result in HDFS path /user/<YOUR_USER_ID>/solutions/solution01/crimes_by_type_by_month
Output File Format: TEXT
Output Columns: Month in YYYYMM format, crime count, crime type
Output Delimiter: \t (tab delimited)
Output Compression: gzip

Solution:


crimeDataRDD = sc.textFile("/public/crime/csv")
header = crimeDataRDD.first()
crimeDataNoHeader = crimeDataRDD.filter(lambda rec: rec != header)

def getYYYYMM(rec):
  data = rec.split(",")
  date = data[2]
  YYYY = date.split(" ")[0][6:]
  MM = date.split(" ")[0][:2]
  crimeType = data[5]
  return(((int(YYYY+MM)), crimeType), 1)

crimeDataMap = crimeDataNoHeader.map(lambda rec: getYYYYMM(rec))
crimeCount = crimeDataMap.reduceByKey(lambda x, y: x + y)
crimeMap = crimeCount.map(lambda rec: ((rec[0][0], -rec[1]), str(rec[0][0]) + "\t" + str(rec[1]) + "\t" + rec[0][1]))
crimeCountSort = crimeMap.sortByKey()
crimeDataSorted = crimeCountSort.map(lambda rec: rec[1])
crimeDataSorted.coalesce(2).saveAsTextFile("/user/bbastola/solutions/solution01/crimes_by_type_by_month", compressionCodecClass="org.apache.hadoop.io.compress.GzipCodec")

