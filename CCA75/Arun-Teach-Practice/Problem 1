Problem 1:
data located at /public/retail_db/orders, /public/retail_db/order_items
Using sqoop, import orders table into hdfs to folders /user/bbastola/problem1/orders. File should be loaded as Avro File and use gzip compression
Using sqoop, import order_items  table into hdfs to folders /user/bbastola/problem1/order-items. Files should be loaded as avro file and use snappy compression

Using pyspark load data at /user/cloudera/problem1/orders and /user/cloudera/problem1/orders-items items as dataframes. 
Expected Intermediate Result: Order_Date , Order_status, total_orders, total_amount. In plain english, please find total orders and total amount per status per day. The result should be sorted by order date in descending, order status in ascending and total amount in descending and total orders in ascending. Aggregation should be done using below methods. However, sorting can be done using a dataframe or RDD. Perform aggregation in each of the following ways

b). Using Spark SQL  - here order_date should be YYYY-MM-DD format
c). By using combineByKey function on RDDS -- No need of formatting order_date or total_amount

 Store the result as parquet file into hdfs using gzip compression under folder

/user/cloudera/problem1/result4b-gzip
/user/cloudera/problem1/result4c-gzip
 Store the result as parquet file into hdfs using snappy compression under folder

/user/cloudera/problem1/result4b-snappy
/user/cloudera/problem1/result4c-snappy

Store the result as CSV file into hdfs using No compression under folder

/user/cloudera/problem1/result4b-csv
/user/cloudera/problem1/result4c-csv

create a mysql table named result and load data from /user/cloudera/problem1/result4a-csv to mysql table named result 

//Solutions

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem1/orders \
--as-avrodatafile \
--compress 

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username retail_dba \
--password cloudera \
--table order_items \
--target-dir /user/cloudera/problem1/order-items \
--as-avrodatafile \
--compress \
--compression-codec org.apacha.hadoop.io.compress.SnappyCodec

4c)
ordersDF = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem1/orders")
orderItemsDF = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/problem1/order-items")

ordersRDD = ordersDF.map(list)
orderItemsRDD = orderItemsDF.map(list)

ordersRDDMap = ordersRDD.map(lambda rec: (int(rec[0]),(str(rec[1]), rec[3])))
orderItemsRDDMap = orderItemsRDD.map(lambda rec: (int(rec[1]), round(float(rec[4]))))
ordersJoinOrderItems = ordersRDDMap.join(orderItemsRDDMap)
ordersJoinOrderItemsMap = ordersJoinOrderItems.map(lambda rec: rec[1])
totalOrderPerStatus = ordersJoinOrderItemsMap.aggregateByKey((0.0, 0), (lambda x, y: (x[0] + y, x[1] + 1)), (lambda a,b: (a[0] + b[0], a[1] + b[1])))
from pyspark.sql import Row
finalOutputDF = totalOrderPerStatus.map(lambda rec: Row(date=rec[0][0], payment_status=rec[0][1], total_orders=rec[1][1], total_Revenue=rec[1][0])).toDF()

sqlContext.setConf("spark.sql.parquet.compression.codec","gzip")
finalOutputDF.write.parquet("/user/cloudera/problem1/result4c-gzip")

sqlContext.setConf("spark.sql.parquet.compression.codec","Snappy")
finalOutputDF.write.parquet("/user/cloudera/problem1/result4c-snappy")

mysql -h localhost -u retail_dba -p cloudera

use database bbastola_export;
create table bbastola_result(
Order_Date varchar(200), 
Order_status varchar(200), 
total_orders double(10), 
total_amount float(10)
)

sqoop export \
--connect jdbc:mysql://localhost:3306/retail_db \
--username retail_dba \
--password cloudera \
--export-dir /user/cloudera/problem1/result4c-csv
--table bbastola_CCA175

