Problem 6: 

Using Spark SQL over Spark SQL Context or by using RDDs

create a hive meta store database named retail_db and import all tables from mysql retail_db database into hive meta store.

sqoop import-all-tables \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--hive-import \
--hive-database retail_db \
--fields-terminated-by "," \
--lines-terminated-by "\n" \
--num-mappers 1

Rank products within department by price and order by department ascending and rank descending [this proves you can produce ranked and sorted data on joined data sets]

departmentsRDD = sc.textFile("/user/hive/warehouse/retail_db.db/departments")
departmentsMap = departmentsRDD.map(lambda rec: (int(rec.split(",")[0]), rec.split(",")[1])) 
categoriesRDD = sc.textFile("/user/hive/warehouse/retail_db.db/categories")
categoriesMap = categoriesRDD.map(lambda rec: (int(rec.split(",")[1]), int(rec.split(",")[0])))
productsRDD = sc.textFile("/user/hive/warehouse/retail_db.db/products")
productsMap = productsRDD.map(lambda rec: (int(rec.split(",")[1]),(rec.split(",")[2], rec.split(",")[4])))

departmentsJoinCategories = departmentsMap.join(categoriesMap).map(lambda rec: (int(rec[1][1]), rec[1][0]))
allTables = productsMap.union(departmentsJoinCategories)
allTablesMap = allTables.map(lambda rec: (rec[1][1], (rec[1][0][1], rec[1][0][0])))
tablesGroupBy = allTablesMap.groupByKey()


find top 10 customers with most unique product purchases. if more than one customer has the same number of product purchases then the customer with the lowest customer_id will take precedence [this proves you can produce aggregate statistics on joined datasets]

On dataset from step 3, apply filter such that only products less than 100 are extracted [this proves you can use subqueries and also filter data]

On dataset from step 4, extract details of products purchased by top 10 customers which are priced at less than 100 USD per unit [this proves you can use subqueries and also filter data]

Store the result of 5 and 6 in new meta store tables within hive. [this proves your ability to use metastore as a sink]

departmentsRDD = sc.textFile("/user/hive/warehouse/retail_db.db/departments") 
categoriesRDD = sc.textFile("/user/hive/warehouse/retail_db.db/categories")
productsRDD = sc.textFile("/user/hive/warehouse/retail_db.db/products")

