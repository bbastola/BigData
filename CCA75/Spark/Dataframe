//Datframe --> distributed collection with structure
           --> Extension to RDD

sqlContext --> Created using sparkContext(sc)
           --> it's of type hiveContext

load --> sqlContext.load("/public/retail_db_json/order_items", "json").show()
read --> sqlContext.read.json("/public/retail_db_json/order_items").show()
 try with orc, parquet, avro

 //creating avro files

 sqoop import \
--options-file ./connection.props \
--table order_items \
--target-dir /user/bbastola/sqoop_import/order_items \
--num-mappers 1 \
--as-avrodatafile

pyspark --master yarn --packages com.databricks:spark-avro_2.10:2.0.1

order_items = sqlContext.read.format("com.databricks.spark.avro").load("/user/bbastola/sqoop_import/order_items/part-m-00000")
order_items.show()


//creating parquet files

 sqoop import \
--options-file ./connection.props \
--table order_items \
--target-dir /user/bbastola/sqoop_import/order_items/parquet \
--num-mappers 1 \
--as-parquetfile

order_items = sqlContext.read.parquet("/user/bbastola/sqoop_import/order_items/parquet/d17fc45a-f5c6-4ce9-b696-3c18fdd13916.parquet").show()

