
* Importing individual table from MySql database to HDFS as avro data file:

sqoop import -Dmapreduce.job.user.classpath.first=true \
--options-file connection.props \
--table categories \
--target-dir="/user/bbastola/Dataset/categories_avro" \
--as-avrodatafile \
--delete-target-dir \
-m 1

* To copy all .avro file from hdfs to local:
hadoop fs -get /user/bbastola/Dataset/categories_avro/* 

* To print out the meta of a given .avro file:

avro-tools getmeta part-m-00000.avro

* To print out the Schema of a given .avro file:

avro-tools getSchema part-m-00000.avro
		********** OR **********
* Also When sqoop command is run to import table as avrodatafile, 
  .avsc file is created by default under the location from where the sqoop was ran.
  This .avsc file contains the metadata of the table we imported. we can do: 
cat categories.avsc to see the metadata

* To Store the meta of a given .avro as .avsc file

avro-tools getmeta part-m-00000.avro > part-m-00000.getmeta.avsc

* To list few records (limit) from a given .avro file

avro-tools cat part-m-00000.avro --limit 2 - 

To copy first 5 records from .avro file to another .avro (new File)
avro-tools cat --offset 1 --limit 5 .avro file first-5-from-part-m-0000.avro // part-m-0000.avro being the .avro file

To copy 6 to 10 records from a .avro file to another .avro (new File)
avro-tools cat --offset 6 --limit 5 .avro file record-6-10-part-m-00000.avro

# To compile a given .avsc into java code
 avro-tools compile -string schema ./schemas ./javacode

