
crimeData = sc.textFile("/public/crime/csv")
header = crimeData.first()
crimeDataFilter = crimeData.filter(lambda rec: rec!=header)

Get monthly count of primary crime type, sorted by month in ascending and number of crimes per type in descending order

from pyspark.sql import Row
crimeDF = crimeDataFilter.map(lambda r: Row(date=r.split(",")[2], crime_type=r.split(",")[5])).toDF()
crimeDF.registerTempTable("crime_table")

crimeAnalysis = sqlContext.sql("select concat(substr(date,7,4), substr(date,1,2)) month ,crime_type, \
count(crime_type) total_count_by_type from crime_table group by concat(substr(date,7,4), substr(date,1,2)), \
crime_type order by month, total_count_by_type desc")

crimeAnalysis.rdd.map(lambda r: r[0]+"\t"+r[2]+"\t"+[1]).saveAsTextFile("/user/bbastola/solutions/solution01_latest/crimes_by_type_by_month","org.apache.hadoop.io.compress.GzipCodec")
