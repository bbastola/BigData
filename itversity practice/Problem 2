orders = open("/data/retail_db/orders/part-00000").read().splitlines()
ordersRDD = sc.parallelize(orders)
customers = open("/data/retail_db/customers/part-00000").read().splitlines()
customersRDD = sc.parallelize(customers)

from pyspark.sql import Row
ordersDF = ordersRDD.map(lambda r: Row(order_customer_id=int(r.split(",")[2]), order_id=int(r.split(",")[0]))).toDF()
customersDF = customersRDD.map(lambda r: Row(customer_id=int(r.split(",")[0]), customer_fname=r.split(",")[1], customer_lname=r.split(",")[2])).toDF()

ordersDF.registerTempTable("orders_table")
customersDF.registerTempTable("customers_table")

Result = sqlContext.sql("select c.customer_lname, c.customer_fname from customers_table c left outer join orders_table o \
on c.customer_id = o.order_customer_id \
where o.order_id is null \
group by c.customer_lname, c.customer_fname \
sort by c.customer_lname, c.customer_fname")

Result.rdd.map(lambda r: (r[0]+ "," + r[1])).coalesce(1).saveAsTextFile("/user/bbastola/solutions/solutions02/inactive_customers_lastest")