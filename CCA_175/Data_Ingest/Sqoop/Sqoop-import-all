Sqoop-import-all

1. Importing all tables from MySql database at once:
(From here onwards I'll be using connetion.props parameter file instead of hard coding host,username,password)

sqoop import-all-tables \
--options-file connection.props \
--warehouse-dir="/user/localhost/retail_db"

2. Importing all tables from Mysql database using compression + excluding few tables

sqoop import-all-tables \
--options-file connection.props \
--warehouse-dir=/user/bbastola/Dataset/ \
--compress \
--compression-codec org.apache.hadoop.io.compress.GzipCodec \
--exclude-tables orders,order_items \
-m 6

//Here we have used exclude-tables argument to excluse two tables(there shoudldn't be any space between two tables) while importing all tables. 
//Here we have used Gzip compression. There are several other compression codecs that we can use:
org.apache.hadoop.io.compress.DefaultCodec
org.apache.hadoop.io.compress.BZip2Codec
org.apache.hadoop.io.compress.SnappyCodec

//You can use the following command to check first 20 records of the result:
hadoop fs -cat /user/bbastola/Dataset/categories/part* | zcat | head -n 20

sqoop_import_all_tables_common_arguments
=======================================
Common arguments

Argument	Description
--connect <jdbc-uri>	Specify JDBC connect string
--connection-manager <class-name>	Specify connection manager class to use
--driver <class-name>	Manually specify JDBC driver class to use
--hadoop-mapred-home <dir>	Override $HADOOP_MAPRED_HOME
--help	Print usage instructions
--password-file	Set path for a file containing the authentication password
-P	Read password from console
--password <password>	Set authentication password
--username <username>	Set authentication username
--verbose	Print more information while working
--connection-param-file <filename>	Optional properties file that provides connection parameters
--relaxed-isolation	Set connection transaction isolation to read uncommitted for the mappers.


Import control arguments:

Argument	Description
--as-avrodatafile	Imports data to Avro Data Files
--as-sequencefile	Imports data to SequenceFiles
--as-textfile	Imports data as plain text (default)
--as-parquetfile	Imports data to Parquet Files
--direct	Use direct import fast path
--inline-lob-limit <n>	Set the maximum size for an inline LOB
-m,--num-mappers <n>	Use n map tasks to import in parallel
--warehouse-dir <dir>	HDFS parent for table destination
-z,--compress	Enable compression
--compression-codec <c>	Use Hadoop codec (default gzip)
--exclude-tables <tables>	Comma separated list of tables to exclude from import process
