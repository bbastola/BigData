sqoop-import-hive

* Importing all tables from mysql into hive:

sqoop import-all-tables \
--connect "jdbc:mysql://localhost:3306/retail_db" \
--username=retail_dba \
-P \
--hive-import \
--hive-database retail_db \
--create-hive-table \
-m 1

//Make sure you have already create retail_db database on hive before importing

dfs -ls /apps/hive/warehouse/retail_db.db;
//This command will let you see your contents from hive without returning hdfs shell

* Importing individual tables:

sqoop import \
--options-file connection.props \
--driver com.mysql.jdbc.Driver \
--table orders \
--hive-import \
--create-hive-table \
--hive-table retail_db.orders \ 
-m 1

Hive arguments:

Argument	Description
--hive-home <dir>	Override $HIVE_HOME
--hive-import	Import tables into Hive (Uses Hiveâ€™s default delimiters if none are set.)
--hive-overwrite	Overwrite existing data in the Hive table.
--create-hive-table	If set, then the job will fail if the target hive
table exits. By default this property is false.
--hive-table <table-name>	Sets the table name to use when importing to Hive.
--hive-drop-import-delims	Drops \n, \r, and \01 from string fields when importing to Hive.
--hive-delims-replacement	Replace \n, \r, and \01 from string fields with user defined string when importing to Hive.
--hive-partition-key	Name of a hive field to partition are sharded on
--hive-partition-value <v>	String-value that serves as partition key for this imported into hive in this job.
--map-column-hive <map>	Override default mapping from SQL type to Hive type for configured columns.
