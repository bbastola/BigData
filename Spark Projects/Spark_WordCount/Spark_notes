creating spark application:

1. go to the folder where your build.sbt and project files are located. 
2. run sbt
3. run console 
4. import org.apache.spark.SparkConf
5. import org.apache.spark.SparkContext
6. val conf = new org.apache.spark.SparkConf().setAppName("Testing").setMaster("local")
7. val sc = new SparkContext(conf)
----------------------------------------------------------------------------------------------------
//examples
var a = sc.parallelize((1 to 100).toList)
var b = a.filter(num => num % 2 == 0) //will get even numbers only
var c = b.count
----------------------------------------------------------------------------------------------------
//study 
google: spark programing guide 1.6.2
 programing guide -> spark programing guide
 study transformation and actions topic: master 15-20 collections apis
----------------------------------------------------------------------------------------------------
//Count total number of order status from orders table

//--Step 1 - loading data(text file) into spark shell -- creating RDD
val orders = sc.textFile("file:///data/retail_db/orders") //if getting data from itversity lab

val orders = sc.textFile("/Volumes/Hadoop/Github/Databases/data/retail_db/orders") //local computer
ordrers.first() //to check the data

val ordersMap = orders.map(rec => {
	(rec.split(",")(3), 1)
})

val orderStatusCount = ordersMap.reduceByKey((occ, value) => occ + value)

orderStatusCount.collect().foreach(println)
----------------------------------------------------------------------------------------------------
//running spark in cluster
spark-shell --master yarn-client
spark-shell --conf spark.ui.port=22222 --master yarn-client //changing the port to custom in case the port is occupied by other users

use case:
word count using data located in HDFS (working direcetly on cluster. So we don't have to import libraries)
1. val randomtext = sc.textFile("/public/randomtextwriter")
2. randomtext.flatMap(rec => rec.split(" ")).map(rec => (rec, 1)).reduceByKey((agg, value) => agg + value).saveAsTextFile("/user/bbastola/newfolder")

once you run the job: you can find tracking url to track the job. (check and see if you can get tracking url when run in local mode vs yarn mode(possible))

to create jar files
1. go to the location directory of the project.
2. type sbt package
3. jar will be created under project/target/scala-2.10/
4. scp to cluster(gatewaynode)
scp wc_2.10-1.0.jar bbastola@gw01.itversity.com:~
5. login to gatewaynode of cluster

6. to run in local mode enter the following

spark-submit \
--class wc \ //class name
--conf spark.ui.port=22222 \ //if you have to specify port number
wc_2.10-1.0.jar /public/randomtextwriter/part-m-00000 /user/bbastola/wcop  //jar filename /argument(input file) /output location

in order to run the above program in production mode (YARN) we have to do the following:
to get typesafe config library:
google scala typesafe config and search sbt => copy librarydependicies and copy it to the build.sbt file

**if we need to find class name then we can type the following:

jar tvf jarfilename

then follow below:
1. create application.properties file under resources that contains following
dev.executionMode = local
prod.executionMode = yarn-client
2. under main funtion type:
val props = ConfigFactory.load().
setMaster(props.getConfig("dev"). getString("executionMode")).
setAppName("Word Count")

to run:
spark-submit \
--class wc \
--master yarn \
--conf spark.ui.port=22222 \ 
wc_2.10-1.0.jar /public/randomtextwriter/part-m-00000 /user/bbastola/wcop prod \\prod argument at the end

need to study executors, executions on spark
